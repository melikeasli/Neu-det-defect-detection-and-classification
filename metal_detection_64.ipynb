{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DETECTION\n",
        "In this section, we first import the necessary libraries. PyTorch is a tool used for creating and training neural networks. NumPy and Pandas are used to efficiently manage large datasets and perform numerical operations. Torchvision.transforms and PIL (Pillow) handle the tasks of loading and pre-processing images (sizing, normalizing). Finally, Matplotlib and Tqdm provide auxiliary functions such as visualization and progress bars."
      ],
      "metadata": {
        "id": "uErQd3RtPIoe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXEtDiAbaUyx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.ops import box_iou, nms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from sklearn.cluster import KMeans\n",
        "import time\n",
        "import copy\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "import cv2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'\\n✓ Device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'✓ GPU: {torch.cuda.get_device_name(0)}')"
      ],
      "metadata": {
        "id": "-AcKMrx8aWO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd91671a-dc84-43a5-9478-dd99c030ace6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Device: cuda\n",
            "✓ GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/neu-surface.zip'\n",
        "extract_path = '/content/neu_dataset'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "dataset_base = os.path.join(extract_path, 'NEU-DET')"
      ],
      "metadata": {
        "id": "Gow4B_wsbPpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In object detection projects, labeling information is typically found in XML files in Pascal VOC format. This function (parse_voc_xml) reads these XML files and parses the structural information within them. For each file, it extracts the image name, its original width and height, and, most importantly, the class label and bounding box coordinates (xmin, ymin, xmax, ymax) of each defect in the image. This is the first step in transforming the original text data into a structured list that the model can understand."
      ],
      "metadata": {
        "id": "Fu12oYJMPQ6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_voc_xml(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Image information\n",
        "    size = root.find('size')\n",
        "    width = int(size.find('width').text)\n",
        "    height = int(size.find('height').text)\n",
        "\n",
        "    filename = root.find('filename').text\n",
        "\n",
        "    # Bounding boxes\n",
        "    boxes = []\n",
        "    labels = []\n",
        "\n",
        "    for obj in root.findall('object'):\n",
        "        label = obj.find('name').text\n",
        "        bbox = obj.find('bndbox')\n",
        "\n",
        "        xmin = int(bbox.find('xmin').text)\n",
        "        ymin = int(bbox.find('ymin').text)\n",
        "        xmax = int(bbox.find('xmax').text)\n",
        "        ymax = int(bbox.find('ymax').text)\n",
        "\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "        labels.append(label)\n",
        "    return {\n",
        "        'filename': filename,\n",
        "        'width': width,\n",
        "        'height': height,\n",
        "        'boxes': boxes,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "# Test: Parse an XML file\n",
        "dataset_base = '/content/neu_dataset/NEU-DET'\n",
        "ann_dir = os.path.join(dataset_base, 'train', 'annotations')\n",
        "\n",
        "sample_xml_files = [f for f in os.listdir(ann_dir) if f.endswith('.xml')]\n",
        "if len(sample_xml_files) > 0:\n",
        "    sample_xml = os.path.join(ann_dir, sample_xml_files[0])\n",
        "    sample_annotation = parse_voc_xml(sample_xml)\n",
        "    print(f\"  Filename: {sample_annotation['filename']}\")\n",
        "    print(f\"  Image size: {sample_annotation['width']}x{sample_annotation['height']}\")\n",
        "    print(f\"  Number of objects: {len(sample_annotation['boxes'])}\")\n",
        "    print(f\"  Labels: {sample_annotation['labels']}\")\n",
        "    print(f\"  Boxes: {sample_annotation['boxes']}\")"
      ],
      "metadata": {
        "id": "-j2gL2ACaaXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd027e07-4e34-499d-fa64-88e58d920f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Filename: rolled-in_scale_71.jpg\n",
            "  Image size: 200x200\n",
            "  Number of objects: 2\n",
            "  Labels: ['rolled-in_scale', 'rolled-in_scale']\n",
            "  Boxes: [[81, 1, 187, 82], [79, 142, 135, 171]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `load_detection_dataset` function loads all structural information of the dataset into memory by scanning the specified index structure, reading each XML file, and matching the path to the corresponding image file. During this process, it automatically identifies all unique error classes in the dataset and assigns an integer index to each class. It also skips instances that throw errors or lack valid tags during loading. As a result, lists containing image paths, box coordinates, and class indices are obtained, ready for use in training and validation sets."
      ],
      "metadata": {
        "id": "a8iXFPuZPhpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_detection_dataset(base_path, split='train'):\n",
        "    annotations_path = os.path.join(base_path, split, 'annotations')\n",
        "    images_path = os.path.join(base_path, split, 'images')\n",
        "\n",
        "    if not os.path.exists(annotations_path):\n",
        "        print(f\"ERROR: {annotations_path} not found!\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\n{split.upper()} set loading...\")\n",
        "    print(f\"  Annotations: {annotations_path}\")\n",
        "    print(f\"  Images: {images_path}\")\n",
        "\n",
        "    # List all XML files\n",
        "    xml_files = [f for f in os.listdir(annotations_path) if f.endswith('.xml')]\n",
        "\n",
        "    print(f\"  Found {len(xml_files)} XML files\")\n",
        "\n",
        "    # We extract class names from file names\n",
        "    all_classes = set()\n",
        "    for xml_file in xml_files:\n",
        "        class_name = '_'.join(xml_file.replace('.xml', '').split('_')[:-1])\n",
        "        all_classes.add(class_name)\n",
        "\n",
        "    class_names = sorted(list(all_classes))\n",
        "    class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "\n",
        "    print(f\"  Classes detected: {class_names}\")\n",
        "\n",
        "    dataset = []\n",
        "    class_counts = {name: 0 for name in class_names}\n",
        "    # Load any XML file\n",
        "    for xml_file in xml_files:\n",
        "        xml_path = os.path.join(annotations_path, xml_file)\n",
        "\n",
        "        try:\n",
        "            # Parce XML\n",
        "            ann = parse_voc_xml(xml_path)\n",
        "\n",
        "            # Remove class from file name\n",
        "            class_name = '_'.join(xml_file.replace('.xml', '').split('_')[:-1])\n",
        "\n",
        "            img_name = ann['filename']\n",
        "\n",
        "            # Image location\n",
        "            img_path = os.path.join(images_path, class_name, img_name)\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                # Try different extensions\n",
        "                img_base = os.path.splitext(img_name)[0]\n",
        "                img_path = None\n",
        "                for ext in ['.bmp', '.jpg', '.png', '.jpeg']:\n",
        "                    candidate = os.path.join(images_path, class_name, img_base + ext)\n",
        "                    if os.path.exists(candidate):\n",
        "                        img_path = candidate\n",
        "                        break\n",
        "\n",
        "                if img_path is None:\n",
        "                    continue\n",
        "\n",
        "             # Convert labels to index\n",
        "            label_indices = []\n",
        "            for label in ann['labels']:\n",
        "                if label in class_to_idx:\n",
        "                    label_indices.append(class_to_idx[label])\n",
        "\n",
        "            if len(label_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            dataset.append({\n",
        "                'image_path': img_path,\n",
        "                'boxes': torch.FloatTensor(ann['boxes']),  # [N, 4]\n",
        "                'labels': torch.LongTensor(label_indices),  # [N]\n",
        "                'width': ann['width'],\n",
        "                'height': ann['height'],\n",
        "                'class_name': class_name\n",
        "            })\n",
        "\n",
        "            class_counts[class_name] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Error loading {xml_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n  Loaded samples per class:\")\n",
        "    for class_name in class_names:\n",
        "        print(f\"    {class_name}: {class_counts[class_name]}\")\n",
        "\n",
        "    return dataset, class_names\n",
        "# Load train and validation sets\n",
        "train_dataset, class_names = load_detection_dataset(dataset_base, 'train')\n",
        "val_dataset, _ = load_detection_dataset(dataset_base, 'validation')\n",
        "\n",
        "if train_dataset is not None:\n",
        "    print(f\"\\n✓ Train samples: {len(train_dataset)}\")\n",
        "    print(f\"✓ Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"✓ Classes: {class_names}\")"
      ],
      "metadata": {
        "id": "WVk5OQ6ebaFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c31d04-f4c7-4f5a-b511-a310bf4ad67a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TRAIN set loading...\n",
            "  Annotations: /content/neu_dataset/NEU-DET/train/annotations\n",
            "  Images: /content/neu_dataset/NEU-DET/train/images\n",
            "  Found 1439 XML files\n",
            "  Classes detected: ['crazing', 'inclusion', 'patches', 'pitted_surface', 'rolled-in_scale', 'scratches']\n",
            "\n",
            "  Loaded samples per class:\n",
            "    crazing: 239\n",
            "    inclusion: 240\n",
            "    patches: 240\n",
            "    pitted_surface: 240\n",
            "    rolled-in_scale: 240\n",
            "    scratches: 240\n",
            "\n",
            "VALIDATION set loading...\n",
            "  Annotations: /content/neu_dataset/NEU-DET/validation/annotations\n",
            "  Images: /content/neu_dataset/NEU-DET/validation/images\n",
            "  Found 361 XML files\n",
            "  Classes detected: ['crazing', 'inclusion', 'patches', 'pitted_surface', 'rolled-in_scale', 'scratches']\n",
            "\n",
            "  Loaded samples per class:\n",
            "    crazing: 60\n",
            "    inclusion: 60\n",
            "    patches: 60\n",
            "    pitted_surface: 60\n",
            "    rolled-in_scale: 60\n",
            "    scratches: 60\n",
            "\n",
            "✓ Train samples: 1439\n",
            "✓ Validation samples: 360\n",
            "✓ Classes: ['crazing', 'inclusion', 'patches', 'pitted_surface', 'rolled-in_scale', 'scratches']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the characteristics of the dataset before starting the training is crucial for model selection. This analysis examines the total number of boxes, the average number of errors per image, and the distribution of these errors. Additionally, the number of times each error class recurs (class imbalance) and the average area, width, and height of the detection boxes are calculated. These statistics help us use methods like weighting if there are too few or too few classes that would strain the model."
      ],
      "metadata": {
        "id": "XED7NhCAQLPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_annotations(dataset, class_names):\n",
        "        total_boxes = 0\n",
        "        boxes_per_image = []\n",
        "        class_counts = {i: 0 for i in range(len(class_names))}\n",
        "        box_sizes = []\n",
        "\n",
        "        for sample in dataset:\n",
        "            n_boxes = len(sample['boxes'])\n",
        "            total_boxes += n_boxes\n",
        "            boxes_per_image.append(n_boxes)\n",
        "\n",
        "            # Class distribution\n",
        "            for label in sample['labels']:\n",
        "                class_counts[label.item()] += 1\n",
        "\n",
        "            # Box dimensions\n",
        "            boxes = sample['boxes']\n",
        "            widths = boxes[:, 2] - boxes[:, 0]\n",
        "            heights = boxes[:, 3] - boxes[:, 1]\n",
        "            areas = widths * heights\n",
        "            box_sizes.extend(areas.tolist())\n",
        "\n",
        "        print(f\"\\nTotal images: {len(dataset)}\")\n",
        "        print(f\"Total boxes: {total_boxes}\")\n",
        "        print(f\"Boxes per image: {np.mean(boxes_per_image):.2f} ± {np.std(boxes_per_image):.2f}\")\n",
        "        print(f\"Min boxes: {min(boxes_per_image)}, Max boxes: {max(boxes_per_image)}\")\n",
        "\n",
        "        print(f\"\\nClass distribution:\")\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            print(f\"  {class_name}: {class_counts[i]} boxes\")\n",
        "\n",
        "        print(f\"\\nBox sizes (area):\")\n",
        "        print(f\"  Mean: {np.mean(box_sizes):.1f} pixels²\")\n",
        "        print(f\"  Std: {np.std(box_sizes):.1f} pixels²\")\n",
        "        print(f\"  Min: {min(box_sizes):.1f}, Max: {max(box_sizes):.1f}\")\n",
        "\n",
        "analyze_annotations(train_dataset, class_names)"
      ],
      "metadata": {
        "id": "MLewPOVhbil7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796aa4d4-60b9-4876-9c8e-0e69ef62e8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total images: 1439\n",
            "Total boxes: 3332\n",
            "Boxes per image: 2.32 ± 1.28\n",
            "Min boxes: 1, Max boxes: 9\n",
            "\n",
            "Class distribution:\n",
            "  crazing: 524 boxes\n",
            "  inclusion: 852 boxes\n",
            "  patches: 688 boxes\n",
            "  pitted_surface: 345 boxes\n",
            "  rolled-in_scale: 496 boxes\n",
            "  scratches: 427 boxes\n",
            "\n",
            "Box sizes (area):\n",
            "  Mean: 6856.0 pixels²\n",
            "  Std: 7633.7 pixels²\n",
            "  Min: 108.0, Max: 39601.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NEUDetectionDataset class uses PyTorch's data loading and preprocessing logic. The `__getitem__` method, when called, loads the image from disk and retrieves the corresponding box/label data. Images are resized to the specified input dimensions (416x416), and box coordinates are normalized to this size (set to a range of 0-1). Additionally, if used for training, arbitrary data augmentation techniques (rotation, brightness adjustment) are applied to images and boxes to improve the model's generalization capabilities."
      ],
      "metadata": {
        "id": "6FcN-_4_RONg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NEUDetectionDataset(Dataset):\n",
        "    def __init__(self, dataset, class_names, img_size=416, augment=False):\n",
        "        self.dataset = dataset\n",
        "        self.class_names = class_names\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        self.num_classes = len(class_names)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(sample['image_path']).convert('RGB')\n",
        "        boxes = sample['boxes'].clone()  # [N, 4]\n",
        "        labels = sample['labels'].clone()  # [N]\n",
        "\n",
        "        original_width = sample['width']\n",
        "        original_height = sample['height']\n",
        "\n",
        "        # Apply augmentation if training\n",
        "        if self.augment:\n",
        "            image, boxes = self.apply_augmentation(image, boxes, original_width, original_height)\n",
        "\n",
        "        # Resize image\n",
        "        image = image.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
        "\n",
        "        # Normalize boxes to [0, 1] relative to ORIGINAL size\n",
        "        if len(boxes) > 0:\n",
        "            boxes[:, [0, 2]] /= original_width   # x coordinates\n",
        "            boxes[:, [1, 3]] /= original_height  # y coordinates\n",
        "            boxes = torch.clamp(boxes, 0, 1)\n",
        "\n",
        "        # Convert to tensor\n",
        "        image = TF.to_tensor(image)\n",
        "\n",
        "        # Normalize\n",
        "        image = TF.normalize(image, mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225])\n",
        "        return image, boxes, labels\n",
        "\n",
        "    def apply_augmentation(self, image, boxes, width, height):\n",
        "        if random.random() > 0.4:\n",
        "            scale_factor = random.uniform(1.0, 1.1)\n",
        "            new_width = int(width * scale_factor)\n",
        "            new_height = int(height * scale_factor)\n",
        "\n",
        "            image_scaled = image.resize((new_width, new_height), Image.BILINEAR)\n",
        "\n",
        "            max_x = new_width - width\n",
        "            max_y = new_height - height\n",
        "            crop_x = random.randint(0, max_x)\n",
        "            crop_y = random.randint(0, max_y)\n",
        "\n",
        "            image = image_scaled.crop((crop_x, crop_y, crop_x + width, crop_y + height))\n",
        "\n",
        "            if len(boxes) > 0:\n",
        "                boxes *= scale_factor\n",
        "\n",
        "                boxes[:, [0, 2]] -= crop_x\n",
        "                boxes[:, [1, 3]] -= crop_y\n",
        "\n",
        "                boxes = torch.clamp(boxes, 0, width)\n",
        "                valid_mask = (boxes[:, 2] - boxes[:, 0] > 1) & (boxes[:, 3] - boxes[:, 1] > 1)\n",
        "                boxes = boxes[valid_mask]\n",
        "                if boxes.numel() == 0:\n",
        "                     boxes = torch.empty((0, 4), dtype=torch.float32)\n",
        "\n",
        "        # Horizontal flip\n",
        "        if random.random() > 0.5:\n",
        "            image = TF.hflip(image)\n",
        "            if len(boxes) > 0:\n",
        "                boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n",
        "        # Vertical flip\n",
        "        if random.random() > 0.5:\n",
        "            image = TF.vflip(image)\n",
        "            if len(boxes) > 0:\n",
        "                boxes[:, [1, 3]] = height - boxes[:, [3, 1]]\n",
        "        # Brightness & contrast\n",
        "        if random.random() > 0.3:\n",
        "            image = TF.adjust_brightness(image, random.uniform(0.85, 1.15))\n",
        "            image = TF.adjust_contrast(image, random.uniform(0.85, 1.15))\n",
        "\n",
        "        if random.random() > 0.3:\n",
        "            image = TF.adjust_gamma(image, gamma=random.uniform(0.8, 1.2))\n",
        "        if random.random() > 0.3:\n",
        "            image = TF.gaussian_blur(image, kernel_size=3)\n",
        "        if random.random() > 0.3:\n",
        "           image = TF.adjust_saturation(image, random.uniform(0.85, 1.15))\n",
        "\n",
        "        return image, boxes\n"
      ],
      "metadata": {
        "id": "PGDalTrlb9cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning models typically process data in batches, but each image in a batch might have a different number of error boxes. The `detection_collate_fn` function solves this problem by stacking all images in a batch while keeping the lists of boxes and tags of varying lengths as separate Python lists."
      ],
      "metadata": {
        "id": "TvORGFQHRosA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detection_collate_fn(batch):\n",
        "    images = []\n",
        "    boxes = []\n",
        "    labels = []\n",
        "\n",
        "    for image, box, label in batch:\n",
        "        images.append(image)\n",
        "        boxes.append(box)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Stack images\n",
        "    images = torch.stack(images, dim=0)\n",
        "\n",
        "    return images, boxes, labels"
      ],
      "metadata": {
        "id": "_hyae580cDkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, using the NEUDetectionDataset class we defined, two separate sets are created from the actual data lists. For the train_det_dataset, augment=True is set, ensuring the model sees slightly modified versions of the images in each training round. For the val_det_dataset, augment=False is set; the images are not subjected to any random changes since a validation test is being performed. This distinction is used to accurately measure the model's true performance and generalization ability."
      ],
      "metadata": {
        "id": "p096OXjFSBNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 416\n",
        "# Create train dataset (with augmentation)\n",
        "train_det_dataset = NEUDetectionDataset(\n",
        "    dataset=train_dataset,\n",
        "    class_names=class_names,\n",
        "    img_size=IMG_SIZE,\n",
        "    augment=True,  # Training augmentation\n",
        ")\n",
        "\n",
        "# Create validation dataset (no augmentation)\n",
        "val_det_dataset = NEUDetectionDataset(\n",
        "    dataset=val_dataset,\n",
        "    class_names=class_names,\n",
        "    img_size=IMG_SIZE,\n",
        "    augment=False  # No augmentation for validation\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Train Detection Dataset: {len(train_det_dataset)} images\")\n",
        "print(f\"✓ Validation Detection Dataset: {len(val_det_dataset)} images\")\n",
        "print(f\"✓ Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"✓ Augmentation: Horizontal flip, Vertical flip, Color jitter\")"
      ],
      "metadata": {
        "id": "7kKlQ6AecIM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36c2ba0-f8a0-46a7-8c88-c633d04c5819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Train Detection Dataset: 1439 images\n",
            "✓ Validation Detection Dataset: 360 images\n",
            "✓ Image size: 416x416\n",
            "✓ Augmentation: Horizontal flip, Vertical flip, Color jitter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader objects are responsible for efficiently extracting data packets (batches) from the dataset. Setting BATCH_SIZE=8 means the model will process 8 images at each step. For the training loader, setting shuffle=True prevents the model from memorizing. The settings pin_memory=True and num_workers=0 (due to environmental constraints) attempt to optimize data transfer. These loaders ensure the training cycle receives data quickly."
      ],
      "metadata": {
        "id": "Lk842VPWSWhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "train_det_loader = DataLoader(\n",
        "    train_det_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=detection_collate_fn,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_det_loader = DataLoader(\n",
        "    val_det_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=detection_collate_fn,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain DataLoader:\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  - Total batches: {len(train_det_loader)}\")\n",
        "print(f\"  - Augmentation: ON\")\n",
        "\n",
        "print(f\"\\nValidation DataLoader:\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  - Total batches: {len(val_det_loader)}\")\n",
        "print(f\"  - Augmentation: OFF\")"
      ],
      "metadata": {
        "id": "WPa1APGNcW0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8675995a-56da-4daf-8b21-32659ed0d6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train DataLoader:\n",
            "  - Batch size: 8\n",
            "  - Total batches: 180\n",
            "  - Augmentation: ON\n",
            "\n",
            "Validation DataLoader:\n",
            "  - Batch size: 8\n",
            "  - Total batches: 45\n",
            "  - Augmentation: OFF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training the model, a test is performed to ensure the data pipeline is working correctly. The first data packet is retrieved from the training loader using the `next(iter(train_det_loader))` command. The dimensions of the retrieved images are checked to ensure they are [8, 3, 416, 416] (batch, channel, height, width). Additionally, the bin coordinates and label indices for each image within the packet are examined; the bins should be normalized between 0 and 1. This step helps detect data formatting errors before training begins."
      ],
      "metadata": {
        "id": "HKCp0ew3SjpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test dataloader\n",
        "# Get one batch\n",
        "images, boxes_list, labels_list = next(iter(train_det_loader))\n",
        "\n",
        "print(f\"  - Images shape: {images.shape}\")\n",
        "print(f\"  - Number of images: {len(boxes_list)}\")\n",
        "\n",
        "for i in range(min(3, len(boxes_list))):\n",
        "    print(f\"\\n  Image {i+1}:\")\n",
        "    print(f\"    - Number of objects: {len(boxes_list[i])}\")\n",
        "    if len(boxes_list[i]) > 0:\n",
        "        print(f\"    - Box shape: {boxes_list[i].shape}\")\n",
        "        print(f\"    - Labels: {labels_list[i].tolist()}\")\n",
        "        print(f\"    - Boxes (normalized): {boxes_list[i][:2].tolist()}\")  # First 2 boxes\n"
      ],
      "metadata": {
        "id": "TKXcjC8PcbME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55359e0-a752-441e-ac0e-cb7e1f6886bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Images shape: torch.Size([8, 3, 416, 416])\n",
            "  - Number of images: 8\n",
            "\n",
            "  Image 1:\n",
            "    - Number of objects: 3\n",
            "    - Box shape: torch.Size([3, 4])\n",
            "    - Labels: [2, 2, 2]\n",
            "    - Boxes (normalized): [[0.2199999988079071, 0.699999988079071, 0.4650000035762787, 0.9900000095367432], [0.7049999833106995, 0.6349999904632568, 0.9150000214576721, 0.8550000190734863]]\n",
            "\n",
            "  Image 2:\n",
            "    - Number of objects: 2\n",
            "    - Box shape: torch.Size([2, 4])\n",
            "    - Labels: [1, 1]\n",
            "    - Boxes (normalized): [[0.2150000035762787, 0.1850000023841858, 0.38999998569488525, 0.7850000262260437], [0.8100000023841858, 0.3700000047683716, 0.925000011920929, 0.574999988079071]]\n",
            "\n",
            "  Image 3:\n",
            "    - Number of objects: 1\n",
            "    - Box shape: torch.Size([1, 4])\n",
            "    - Labels: [5]\n",
            "    - Boxes (normalized): [[0.7482237815856934, 0.3466154932975769, 0.8174178600311279, 1.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DefectDetector model is essentially a convolutional neural network (CNN) architecture. The DetectionBackbone consists of successive convolution and pooling layers; it hierarchically extracts complex features such as imperfections and textures from the image. This architecture reduces the input from a 416x416 grid to a 13x13 feature map. DetectionHead takes this condensed feature map and makes the final predictions necessary for object detection: box coordinates, object confidence score, and class probabilities. The model makes predictions based on multiple anchor boxes for each 13x13 grid cell."
      ],
      "metadata": {
        "id": "Tcv3V2dGSuNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectionBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 416 -> 208\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        # 208 -> 104\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        # 104 -> 52\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        # 52 -> 26\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.dropout_4 = nn.Dropout(p=0.2)\n",
        "        # 26 -> 13\n",
        "        self.block5 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)  # 208\n",
        "        x = self.block2(x)  # 104\n",
        "        x = self.block3(x)  # 52\n",
        "        x = self.block4(x)  #26\n",
        "        x = self.dropout_4(x)\n",
        "        x = self.block5(x)  # 13\n",
        "        return x\n",
        "\n",
        "class DetectionHead(nn.Module):\n",
        "    def __init__(self, in_channels=512, num_classes=6, num_anchors=6):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        out_channels = num_anchors * (5 + num_classes)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, out_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        B, OC, H, W = out.shape\n",
        "        A = OC // (5 + self.num_classes)\n",
        "        out = out.view(B, A, 5 + self.num_classes, H, W)\n",
        "        return out.permute(0, 1, 3, 4, 2).contiguous()  # (B, A, H, W, 5+C)\n",
        "\n",
        "class DefectDetector(nn.Module):\n",
        "    def __init__(self, num_classes, num_anchors=6, anchors=None):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_anchors = num_anchors\n",
        "        self.backbone = DetectionBackbone()\n",
        "        self.detection_head = DetectionHead(in_channels=512, num_classes=num_classes, num_anchors=num_anchors)\n",
        "        if anchors is not None:\n",
        "            self.anchors = anchors\n",
        "        else:\n",
        "            self.anchors = torch.FloatTensor([[0.1, 0.1], [0.2, 0.2], [0.4, 0.4]])\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                if m.weight is not None: nn.init.constant_(m.weight, 1)\n",
        "                if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)\n",
        "        preds = self.detection_head(feats)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "e79KD4uPcxie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An instance of the DefectDetector class model is created and moved to the device (GPU or CPU) designated for training. In this step, the model's structure is checked, and the total number of parameters and the number of trainable parameters are calculated. These numbers provide information about the model's complexity and memory/processing power requirements. For example, 6.19 million parameters indicates that the model is of medium size by deep learning standards."
      ],
      "metadata": {
        "id": "kP7ePf0tSwH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DefectDetector(\n",
        "    num_classes=len(class_names),\n",
        "    num_anchors=6\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Model summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"  - Total parameters: {total_params:,}\")\n",
        "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  - Model size: {total_params*4/1024/1024:.2f} MB\")"
      ],
      "metadata": {
        "id": "Vqnb6Ow_c7T6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a21d16-ea75-41bf-afd5-0eba3421b000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Total parameters: 6,190,722\n",
            "  - Trainable parameters: 6,190,722\n",
            "  - Model size: 23.62 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dummy input (torch.randn(2, 3, 416, 416)) is used to check the model's logical output. This input allows the model to complete a training step. The dimensions of the output tensor are checked: [2, 6, 6, 6, 11]. These dimensions confirm that there are 2 images, 6 anchor boxes, a 6x6 grid size, and 11 predictor values ​​for each box (4 coordinates, 1 confidence score, 6 class probabilities). This test is important for checking the output fit between the model and the loss function."
      ],
      "metadata": {
        "id": "E14RiSSMTnBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy input\n",
        "test_input = torch.randn(2, 3, 416, 416).to(device)\n",
        "print(f\"\\nTest input shape: {test_input.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(test_input)\n",
        "\n",
        "print(f\"Predictions shape: {predictions.shape}\")\n",
        "print(f\"  - Format: (batch, anchors, grid_h, grid_w, 4+classes)\")\n",
        "print(f\"  - Batch: {predictions.shape[0]}\")\n",
        "print(f\"  - Anchors: {predictions.shape[1]}\")\n",
        "print(f\"  - Grid: {predictions.shape[2]}x{predictions.shape[3]}\")\n",
        "print(f\"  - Predictions per cell: {predictions.shape[4]} (4 box + 1 obj + {len(class_names)} classes)\")"
      ],
      "metadata": {
        "id": "tFFYIIVJc-vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d95da5-b225-427f-800e-b43b75a44ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test input shape: torch.Size([2, 3, 416, 416])\n",
            "Predictions shape: torch.Size([2, 6, 6, 6, 11])\n",
            "  - Format: (batch, anchors, grid_h, grid_w, 4+classes)\n",
            "  - Batch: 2\n",
            "  - Anchors: 6\n",
            "  - Grid: 6x6\n",
            "  - Predictions per cell: 11 (4 box + 1 obj + 6 classes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In YOLO-like detection models, the dimensions of predefined anchor boxes directly affect the model's performance. The get_anchors function uses a K-Means clustering algorithm by summing the widths and heights of all actual fault boxes in the training dataset. This algorithm finds K=6 anchor boxes (width and height ratios) that best represent the shapes of the faults in the dataset."
      ],
      "metadata": {
        "id": "GgXxvkfmTzuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_anchors(dataset, k=6, img_size=416):\n",
        "    wh = []\n",
        "    for sample in dataset:\n",
        "        boxes = sample['boxes']\n",
        "        widths = (boxes[:,2] - boxes[:,0]) * img_size\n",
        "        heights = (boxes[:,3] - boxes[:,1]) * img_size\n",
        "        for w,h in zip(widths, heights):\n",
        "            wh.append([w.item(), h.item()])\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0).fit(wh)\n",
        "    anchors = kmeans.cluster_centers_ / img_size\n",
        "    return torch.tensor(anchors, dtype=torch.float32)\n",
        "\n",
        "def suggest_anchors(dataset, img_size=416):\n",
        "    widths, heights = [], []\n",
        "    for sample in dataset:\n",
        "        boxes = sample['boxes']\n",
        "        widths.extend(((boxes[:,2] - boxes[:,0]) * img_size).tolist())\n",
        "        heights.extend(((boxes[:,3] - boxes[:,1]) * img_size).tolist())\n",
        "    print(\"Mean width:\", np.mean(widths), \"Mean height:\", np.mean(heights))\n",
        "    print(\"Min width:\", np.min(widths), \"Max width:\", np.max(widths))\n",
        "    print(\"Min height:\", np.min(heights), \"Max height:\", np.max(heights))\n",
        "\n",
        "anchors = get_anchors(train_dataset, k=6)\n",
        "print(\"Optimized anchors:\", anchors)\n",
        "\n",
        "suggest_anchors(train_dataset)\n",
        "\n",
        "model = DefectDetector(num_classes=len(class_names), num_anchors=6, anchors=anchors)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "cB91TnbbdLBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e31bec-77ca-46df-b92f-6e671f21ceef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized anchors: tensor([[ 44.4576, 184.1897],\n",
            "        [ 32.0082,  48.9360],\n",
            "        [163.2500, 183.7593],\n",
            "        [161.4216,  65.2648],\n",
            "        [ 91.6648,  67.4757],\n",
            "        [ 48.7271, 111.8719]])\n",
            "Mean width: 27657.7575030012 Mean height: 40685.47418967587\n",
            "Min width: 3328.0 Max width: 82784.0\n",
            "Min height: 3744.0 Max height: 82784.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, the raw prediction values ​​produced by the model in (tx, ty, tw, th) format are converted into real coordinates on the image. The decode_predictions function converts the predictions into normalized (x1, y1, x2, y2) coordinates using grid cell offsets and anchor dimensions with the help of sigmoid and exponential functions. It also multiplies the object confidence score by the class probability to find the final confidence score and retains predictions above a defined threshold."
      ],
      "metadata": {
        "id": "n6MPPn3mT1XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_predictions(predictions, anchors, confidence_threshold=0.2):\n",
        "    B, A, GH, GW, D = predictions.shape\n",
        "    anchors = anchors.to(predictions.device)\n",
        "    all_boxes, all_scores, all_labels = [], [], []\n",
        "\n",
        "    for b in range(B):\n",
        "        pred = predictions[b]\n",
        "        box_pred = pred[..., :4]\n",
        "        obj_pred = pred[..., 4]\n",
        "        class_pred = pred[..., 5:]\n",
        "\n",
        "        obj_score = torch.sigmoid(obj_pred)\n",
        "        class_scores = F.softmax(class_pred, dim=-1)\n",
        "        max_scores, class_indices = torch.max(class_scores, dim=-1)\n",
        "        conf = obj_score * max_scores\n",
        "\n",
        "        gy, gx = torch.meshgrid(\n",
        "            torch.arange(GH, device=predictions.device),\n",
        "            torch.arange(GW, device=predictions.device),\n",
        "            indexing='ij'\n",
        "        )\n",
        "\n",
        "        boxes, scores, labels = [], [], []\n",
        "        for a in range(A):\n",
        "            bx = (torch.sigmoid(box_pred[a, ..., 0]) + gx.float()) / GW\n",
        "            by = (torch.sigmoid(box_pred[a, ..., 1]) + gy.float()) / GH\n",
        "            bw = torch.exp(box_pred[a, ..., 2]) * anchors[a, 0]\n",
        "            bh = torch.exp(box_pred[a, ..., 3]) * anchors[a, 1]\n",
        "\n",
        "            x1 = bx - bw / 2\n",
        "            y1 = by - bh / 2\n",
        "            x2 = bx + bw / 2\n",
        "            y2 = by + bh / 2\n",
        "\n",
        "            box_coords = torch.stack([x1, y1, x2, y2], dim=-1).view(-1, 4)\n",
        "            conf_a = conf[a].view(-1)\n",
        "            cls_a = class_indices[a].view(-1)\n",
        "\n",
        "            mask = conf_a > confidence_threshold\n",
        "            if mask.any():\n",
        "                boxes.append(box_coords[mask])\n",
        "                scores.append(conf_a[mask])\n",
        "                labels.append(cls_a[mask])\n",
        "\n",
        "        if len(boxes) > 0:\n",
        "            all_boxes.append(torch.cat(boxes, dim=0))\n",
        "            all_scores.append(torch.cat(scores, dim=0))\n",
        "            all_labels.append(torch.cat(labels, dim=0))\n",
        "        else:\n",
        "            dev = predictions.device\n",
        "            all_boxes.append(torch.zeros((0, 4), device=dev))\n",
        "            all_scores.append(torch.zeros((0,), device=dev))\n",
        "            all_labels.append(torch.zeros((0,), dtype=torch.long, device=dev))\n",
        "\n",
        "    return all_boxes, all_scores, all_labels\n",
        "\n",
        "def apply_nms(boxes, scores, labels, iou_threshold=0.5):\n",
        "    if len(boxes) == 0:\n",
        "        return boxes, scores, labels\n",
        "    keep_b, keep_s, keep_l = [], [], []\n",
        "    for cls in labels.unique(sorted=True):\n",
        "        mask = labels == cls\n",
        "        cb, cs = boxes[mask], scores[mask]\n",
        "        if cb.numel() == 0: continue\n",
        "        idx = nms(cb, cs, iou_threshold)\n",
        "        keep_b.append(cb[idx]); keep_s.append(cs[idx])\n",
        "        keep_l.append(torch.full((len(idx),), int(cls.item()), dtype=torch.long, device=labels.device))\n",
        "    if len(keep_b) == 0:\n",
        "        dev = labels.device\n",
        "        return torch.zeros((0, 4), device=dev), torch.zeros((0,), device=dev), torch.zeros((0,), dtype=torch.long, device=dev)\n",
        "    return torch.cat(keep_b), torch.cat(keep_s), torch.cat(keep_l)\n"
      ],
      "metadata": {
        "id": "Rrax5MzSd2sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DetectionLoss class measures the error between the values ​​predicted by the model and the actual labels. Total loss is a weighted sum of three main components: Box Loss, which uses MSE for the error in correctly finding coordinates; Object Loss, which uses BCE for the error in correctly predicting whether a box contains an object; and Class Loss, which uses Cross-Entropy for the error in correctly predicting the type of error. Hyperparameters like lambda_coord and lambda_noobj adjust the importance of the loss components, while class weights attempt to correct unbalanced data."
      ],
      "metadata": {
        "id": "rJ4B8iWQUja0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectionLoss(nn.Module):\n",
        "    def __init__(self, num_classes, anchors, lambda_coord=5.0, lambda_noobj=0.3, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.lambda_coord = lambda_coord\n",
        "        self.lambda_noobj = lambda_noobj\n",
        "        self.mse_loss = nn.MSELoss(reduction='sum')\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "        self.ce_loss = nn.CrossEntropyLoss(reduction='sum')\n",
        "        self.anchors = anchors\n",
        "\n",
        "        self.ce_loss = nn.CrossEntropyLoss(\n",
        "            weight=class_weights.to(device) if class_weights is not None else None,\n",
        "            reduction='sum'\n",
        "        )\n",
        "\n",
        "    def forward(self, predictions, targets_boxes, targets_labels):\n",
        "        B, A, GH, GW, D = predictions.shape\n",
        "        assert D == 5 + self.num_classes, f\"Prediction last dim {D} != 5+{self.num_classes}\"\n",
        "        device = predictions.device\n",
        "        anchors = self.anchors.to(device)\n",
        "\n",
        "        box_loss = torch.tensor(0.0, device=device)\n",
        "        obj_loss = torch.tensor(0.0, device=device)\n",
        "        cls_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        for b in range(B):\n",
        "            pred = predictions[b]  # (A, GH, GW, 5+C)\n",
        "            t_boxes = targets_boxes[b].to(device)\n",
        "            t_labels = targets_labels[b].to(device)\n",
        "\n",
        "            if t_boxes.size(0) == 0:\n",
        "                obj_loss += self.lambda_noobj * self.bce_loss(pred[..., 4], torch.zeros_like(pred[..., 4]))\n",
        "                continue\n",
        "\n",
        "            # centers in grid\n",
        "            cx = ((t_boxes[:, 0] + t_boxes[:, 2]) / 2.0) * GW\n",
        "            cy = ((t_boxes[:, 1] + t_boxes[:, 3]) / 2.0) * GH\n",
        "            gj = cx.clamp(0, GW - 1).long()\n",
        "            gi = cy.clamp(0, GH - 1).long()\n",
        "\n",
        "            gt_w = (t_boxes[:, 2] - t_boxes[:, 0])  # fractions\n",
        "            gt_h = (t_boxes[:, 3] - t_boxes[:, 1])\n",
        "\n",
        "            obj_mask = torch.zeros(A, GH, GW, device=device, dtype=torch.bool)\n",
        "            noobj_mask = torch.ones(A, GH, GW, device=device, dtype=torch.bool)\n",
        "\n",
        "            for t in range(t_boxes.size(0)):\n",
        "                i, j = gi[t], gj[t]\n",
        "                # best anchor by size\n",
        "                diff = (torch.log(gt_w[t] / (anchors[:, 0] + 1e-9)).abs()\n",
        "                        + torch.log(gt_h[t] / (anchors[:, 1] + 1e-9)).abs())\n",
        "                best_anchor = int(torch.argmin(diff))\n",
        "\n",
        "                obj_mask[best_anchor, i, j] = True\n",
        "                noobj_mask[best_anchor, i, j] = False\n",
        "\n",
        "                pred_vec = pred[best_anchor, i, j]\n",
        "                pred_box = pred_vec[:4]           # (tx, ty, tw, th)\n",
        "                pred_cls = pred_vec[5:]\n",
        "                target_cls = t_labels[t].long()\n",
        "\n",
        "                # xy offsets in cell\n",
        "                off_x = cx[t] - j.float()\n",
        "                off_y = cy[t] - i.float()\n",
        "                target_xy = torch.stack([off_x, off_y], dim=0)\n",
        "                pred_xy = torch.sigmoid(pred_box[:2])\n",
        "\n",
        "                # wh using anchors (fractions)\n",
        "                pred_w = torch.exp(pred_box[2]) * anchors[best_anchor, 0]\n",
        "                pred_h = torch.exp(pred_box[3]) * anchors[best_anchor, 1]\n",
        "                pred_wh = torch.stack([pred_w, pred_h], dim=0)\n",
        "                target_wh = torch.stack([gt_w[t], gt_h[t]], dim=0)\n",
        "\n",
        "                box_loss += self.mse_loss(pred_xy, target_xy) + self.mse_loss(pred_wh, target_wh)\n",
        "                cls_loss += self.ce_loss(pred_cls.unsqueeze(0), target_cls.unsqueeze(0))\n",
        "\n",
        "            obj_pred = pred[..., 4]\n",
        "            if obj_mask.any():\n",
        "                obj_loss += self.bce_loss(obj_pred[obj_mask], torch.ones_like(obj_pred[obj_mask]))\n",
        "            if noobj_mask.any():\n",
        "                obj_loss += self.lambda_noobj * self.bce_loss(obj_pred[noobj_mask], torch.zeros_like(obj_pred[noobj_mask]))\n",
        "\n",
        "        total = (self.lambda_coord * box_loss + obj_loss + cls_loss) / B\n",
        "        return total, {'total': total.item(), 'box': (box_loss / B).item(), 'obj': (obj_loss / B).item(), 'cls': (cls_loss / B).item()}\n"
      ],
      "metadata": {
        "id": "yyM1NuLsd9L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Optimizer (Adam) updates the model's weights based on feedback from the loss function. The Learning Rate (0.001) determines how many steps the model will take in each update. The Scheduler (ReduceLROnPlateau) automatically reduces the learning rate (factor=0.5) if the validation loss does not improve over a certain number of rounds (patience=3), allowing the model to fine-tune. Finally, class weights are included in the Cross-Entropy loss to allow the model to focus more on less represented error classes."
      ],
      "metadata": {
        "id": "mCtNGos0U7uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 5e-4\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.5, 2.0], dtype=torch.float32)\n",
        "criterion = DetectionLoss(\n",
        "    num_classes=len(class_names),\n",
        "    anchors=model.anchors,\n",
        "    lambda_coord=5.0,\n",
        "    lambda_noobj=0.3,\n",
        "    class_weights=class_weights\n",
        ")"
      ],
      "metadata": {
        "id": "OV3Tx6-peDDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `train_one_epoch` function completes a training run by iterating through all batches in the training dataset. In each batch: images are moved to the GPU, optimizer gradients are reset, predictions are extracted from the model, loss is calculated, gradients are propagated back (loss.backward()), and model weights are updated (optimizer.step()). This function tracks the model's learning progress by returning the average values ​​of the training loss (box, object, class). The validate function measures the model's performance on the validation dataset, which it didn't see during training. At this stage, the model is put into model.eval() mode, and gradient calculation is disabled (torch.no_grad()), thus saving memory and preventing weight updates. Just like in the training cycle, the average loss (total, box, obj, cls) across all packets in the validation set is calculated. This loss value is a metric indicating whether the model has memorized (its generalization ability)."
      ],
      "metadata": {
        "id": "mRHIhitoVK3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = total_box = total_obj = total_cls = 0.0\n",
        "    n_batches = 0\n",
        "    for images, boxes_list, labels_list in tqdm(loader, disable=True):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        boxes_list = [b.to(device) for b in boxes_list]\n",
        "        labels_list = [l.to(device).long() for l in labels_list]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images)\n",
        "        B, A, GH, GW, D = preds.shape\n",
        "        assert D == 5 + criterion.num_classes, f\"pred last dim {D} != 5+{criterion.num_classes}\"\n",
        "\n",
        "        loss, d = criterion(preds, boxes_list, labels_list)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += d['total']; total_box += d['box']; total_obj += d['obj']; total_cls += d['cls']\n",
        "        n_batches += 1\n",
        "\n",
        "    return { 'total': total_loss / n_batches, 'box': total_box / n_batches, 'obj': total_obj / n_batches, 'cls': total_cls / n_batches }\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = total_box = total_obj = total_cls = 0.0\n",
        "    n_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for images, boxes_list, labels_list in tqdm(loader, disable=True):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            boxes_list = [b.to(device) for b in boxes_list]\n",
        "            labels_list = [l.to(device).long() for l in labels_list]\n",
        "\n",
        "            preds = model(images)\n",
        "            B, A, GH, GW, D = preds.shape\n",
        "            assert D == 5 + criterion.num_classes, f\"pred last dim {D} != 5+{criterion.num_classes}\"\n",
        "\n",
        "            loss, d = criterion(preds, boxes_list, labels_list)\n",
        "            total_loss += d['total']; total_box += d['box']; total_obj += d['obj']; total_cls += d['cls']\n",
        "            n_batches += 1\n",
        "\n",
        "    return { 'total': total_loss / n_batches, 'box': total_box / n_batches, 'obj': total_obj / n_batches, 'cls': total_cls / n_batches }\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_wts = None\n",
        "epochs_no_improve = 0"
      ],
      "metadata": {
        "id": "rqBeu9U0eLUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `train_detection_model` main loop continuously runs the training and validation steps for a specified total number of epochs. In each epoch, it monitors the validation loss and records the model's weights (best_model_wts) if this loss is the lowest value ever recorded. The Early Stopping mechanism terminates the training loop early if the validation loss does not improve over a specified patience=10. This prevents unnecessary calculations and prevents overfitting of the model."
      ],
      "metadata": {
        "id": "p50Td-7_VYbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_detection_model(model, train_loader, val_loader, criterion,\n",
        "                          optimizer, scheduler, num_epochs, device,\n",
        "                          early_stopping_patience=10):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_box_loss': [],\n",
        "        'train_obj_loss': [],\n",
        "        'train_cls_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_box_loss': [],\n",
        "        'val_obj_loss': [],\n",
        "        'val_cls_loss': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 70)\n",
        "\n",
        "        # Train\n",
        "        train_losses = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # Validate\n",
        "        val_losses = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step(val_losses['total'])\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_losses['total'])\n",
        "        history['train_box_loss'].append(train_losses['box'])\n",
        "        history['train_obj_loss'].append(train_losses['obj'])\n",
        "        history['train_cls_loss'].append(train_losses['cls'])\n",
        "        history['val_loss'].append(val_losses['total'])\n",
        "        history['val_box_loss'].append(val_losses['box'])\n",
        "        history['val_obj_loss'].append(val_losses['obj'])\n",
        "        history['val_cls_loss'].append(val_losses['cls'])\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Train Loss: {train_losses['total']:.4f} \"\n",
        "              f\"(box: {train_losses['box']:.4f}, \"\n",
        "              f\"obj: {train_losses['obj']:.4f}, \"\n",
        "              f\"cls: {train_losses['cls']:.4f})\")\n",
        "\n",
        "        print(f\"Val Loss: {val_losses['total']:.4f} \"\n",
        "              f\"(box: {val_losses['box']:.4f}, \"\n",
        "              f\"obj: {val_losses['obj']:.4f}, \"\n",
        "              f\"cls: {val_losses['cls']:.4f})\")\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_losses['total'] < best_loss:\n",
        "            best_loss = val_losses['total']\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "            print(f\"Best model updated! (Val Loss: {val_losses['total']:.4f})\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"{epochs_no_improve}/{early_stopping_patience} - No improvement\")\n",
        "\n",
        "        # Early stopping\n",
        "        if epochs_no_improve >= early_stopping_patience:\n",
        "            print(f\"\\nEarly Stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining completed: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best Val Loss: {best_loss:.4f}')\n",
        "\n",
        "    # Load best weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "w12wBZjYeOgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 75\n",
        "EARLY_STOP_PATIENCE = 10\n",
        "\n",
        "trained_model, history = train_detection_model(\n",
        "    model=model,\n",
        "    train_loader=train_det_loader,\n",
        "    val_loader=val_det_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=75,\n",
        "    device=device,\n",
        "    early_stopping_patience=10\n",
        ")"
      ],
      "metadata": {
        "id": "nKk2C3JCe6jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022a78ef-31c6-4cb0-8b98-41dc6f2815ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 6847.7146 (box: 1365.9194, obj: 12.8226, cls: 5.2951)\n",
            "Val Loss: 15.6434 (box: 1.1043, obj: 5.6879, cls: 4.4339)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 15.6434)\n",
            "\n",
            "Epoch 2/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 13.6048 (box: 0.8284, obj: 5.5732, cls: 3.8897)\n",
            "Val Loss: 13.8239 (box: 0.8293, obj: 5.4168, cls: 4.2607)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 13.8239)\n",
            "\n",
            "Epoch 3/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 12.6317 (box: 0.7537, obj: 5.3871, cls: 3.4762)\n",
            "Val Loss: 14.0645 (box: 0.8464, obj: 5.2295, cls: 4.6032)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 4/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 12.1653 (box: 0.6925, obj: 5.2820, cls: 3.4209)\n",
            "Val Loss: 13.7242 (box: 0.8761, obj: 5.3421, cls: 4.0014)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 13.7242)\n",
            "\n",
            "Epoch 5/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 11.6431 (box: 0.6663, obj: 5.0521, cls: 3.2593)\n",
            "Val Loss: 13.1100 (box: 0.7414, obj: 5.2238, cls: 4.1791)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 13.1100)\n",
            "\n",
            "Epoch 6/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 10.9490 (box: 0.6448, obj: 4.9148, cls: 2.8101)\n",
            "Val Loss: 11.8952 (box: 0.7446, obj: 4.8693, cls: 3.3026)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 11.8952)\n",
            "\n",
            "Epoch 7/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 10.2751 (box: 0.6568, obj: 4.7202, cls: 2.2711)\n",
            "Val Loss: 12.7490 (box: 0.6635, obj: 4.8768, cls: 4.5547)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 8/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 9.3833 (box: 0.5974, obj: 4.5721, cls: 1.8244)\n",
            "Val Loss: 10.8311 (box: 0.6808, obj: 4.7974, cls: 2.6299)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 10.8311)\n",
            "\n",
            "Epoch 9/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 8.7300 (box: 0.5851, obj: 4.4033, cls: 1.4011)\n",
            "Val Loss: 10.7390 (box: 0.6078, obj: 4.6514, cls: 3.0486)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 10.7390)\n",
            "\n",
            "Epoch 10/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 8.4168 (box: 0.5613, obj: 4.3238, cls: 1.2864)\n",
            "Val Loss: 9.1384 (box: 0.5821, obj: 4.4396, cls: 1.7885)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 9.1384)\n",
            "\n",
            "Epoch 11/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 7.8049 (box: 0.5057, obj: 4.1968, cls: 1.0798)\n",
            "Val Loss: 8.5712 (box: 0.6030, obj: 4.4038, cls: 1.1524)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 8.5712)\n",
            "\n",
            "Epoch 12/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 7.6852 (box: 0.5031, obj: 4.0984, cls: 1.0713)\n",
            "Val Loss: 8.3179 (box: 0.5831, obj: 4.2242, cls: 1.1783)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 8.3179)\n",
            "\n",
            "Epoch 13/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 7.3564 (box: 0.4740, obj: 4.0197, cls: 0.9667)\n",
            "Val Loss: 7.6191 (box: 0.5037, obj: 4.2017, cls: 0.8988)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 7.6191)\n",
            "\n",
            "Epoch 14/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 7.0559 (box: 0.4581, obj: 3.9250, cls: 0.8405)\n",
            "Val Loss: 7.8602 (box: 0.5005, obj: 4.3055, cls: 1.0523)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 15/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 6.8668 (box: 0.4414, obj: 3.8819, cls: 0.7778)\n",
            "Val Loss: 7.5251 (box: 0.4910, obj: 4.1745, cls: 0.8955)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 7.5251)\n",
            "\n",
            "Epoch 16/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 6.6904 (box: 0.4256, obj: 3.8227, cls: 0.7399)\n",
            "Val Loss: 7.2698 (box: 0.4690, obj: 4.2338, cls: 0.6909)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 7.2698)\n",
            "\n",
            "Epoch 17/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 6.7571 (box: 0.4311, obj: 3.8311, cls: 0.7704)\n",
            "Val Loss: 8.4944 (box: 0.4927, obj: 4.2247, cls: 1.8065)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 18/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 6.4876 (box: 0.4108, obj: 3.8170, cls: 0.6164)\n",
            "Val Loss: 6.8436 (box: 0.4399, obj: 3.9850, cls: 0.6590)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 6.8436)\n",
            "\n",
            "Epoch 19/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 6.2773 (box: 0.3996, obj: 3.7242, cls: 0.5552)\n",
            "Val Loss: 7.6150 (box: 0.4356, obj: 4.1068, cls: 1.3302)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 20/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 6.2261 (box: 0.4018, obj: 3.6575, cls: 0.5595)\n",
            "Val Loss: 6.5387 (box: 0.4051, obj: 4.0022, cls: 0.5108)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 6.5387)\n",
            "\n",
            "Epoch 21/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 6.1095 (box: 0.3902, obj: 3.6493, cls: 0.5092)\n",
            "Val Loss: 7.0972 (box: 0.4497, obj: 4.1048, cls: 0.7437)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 22/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 6.1157 (box: 0.3834, obj: 3.6425, cls: 0.5560)\n",
            "Val Loss: 7.5630 (box: 0.5638, obj: 3.8380, cls: 0.9061)\n",
            "LR: 0.001000\n",
            "2/10 - No improvement\n",
            "\n",
            "Epoch 23/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 6.0302 (box: 0.3710, obj: 3.6331, cls: 0.5419)\n",
            "Val Loss: 7.3611 (box: 0.4666, obj: 4.1241, cls: 0.9038)\n",
            "LR: 0.001000\n",
            "3/10 - No improvement\n",
            "\n",
            "Epoch 24/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.7528 (box: 0.3627, obj: 3.5171, cls: 0.4223)\n",
            "Val Loss: 6.5161 (box: 0.3933, obj: 3.9042, cls: 0.6452)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 6.5161)\n",
            "\n",
            "Epoch 25/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.8396 (box: 0.3548, obj: 3.5578, cls: 0.5079)\n",
            "Val Loss: 6.2832 (box: 0.4153, obj: 3.8732, cls: 0.3337)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 6.2832)\n",
            "\n",
            "Epoch 26/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.7468 (box: 0.3551, obj: 3.5464, cls: 0.4249)\n",
            "Val Loss: 6.2142 (box: 0.4348, obj: 3.7535, cls: 0.2868)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 6.2142)\n",
            "\n",
            "Epoch 27/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.6576 (box: 0.3431, obj: 3.5013, cls: 0.4405)\n",
            "Val Loss: 6.5325 (box: 0.4259, obj: 3.8667, cls: 0.5362)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 28/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.5815 (box: 0.3413, obj: 3.4624, cls: 0.4128)\n",
            "Val Loss: 6.3178 (box: 0.4164, obj: 3.8006, cls: 0.4354)\n",
            "LR: 0.001000\n",
            "2/10 - No improvement\n",
            "\n",
            "Epoch 29/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.5519 (box: 0.3376, obj: 3.4617, cls: 0.4022)\n",
            "Val Loss: 5.7911 (box: 0.3602, obj: 3.6651, cls: 0.3248)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 5.7911)\n",
            "\n",
            "Epoch 30/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.4513 (box: 0.3232, obj: 3.4525, cls: 0.3827)\n",
            "Val Loss: 5.8367 (box: 0.3848, obj: 3.6407, cls: 0.2720)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 31/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.4033 (box: 0.3233, obj: 3.4187, cls: 0.3679)\n",
            "Val Loss: 5.7513 (box: 0.3596, obj: 3.6309, cls: 0.3225)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 5.7513)\n",
            "\n",
            "Epoch 32/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.2445 (box: 0.3126, obj: 3.3567, cls: 0.3249)\n",
            "Val Loss: 7.4452 (box: 0.3736, obj: 3.6580, cls: 1.9191)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 33/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.2701 (box: 0.3168, obj: 3.3645, cls: 0.3216)\n",
            "Val Loss: 6.1554 (box: 0.3846, obj: 3.7561, cls: 0.4763)\n",
            "LR: 0.001000\n",
            "2/10 - No improvement\n",
            "\n",
            "Epoch 34/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.1988 (box: 0.3151, obj: 3.2974, cls: 0.3259)\n",
            "Val Loss: 5.7234 (box: 0.3651, obj: 3.6790, cls: 0.2189)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 5.7234)\n",
            "\n",
            "Epoch 35/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.0320 (box: 0.2963, obj: 3.2676, cls: 0.2829)\n",
            "Val Loss: 6.2074 (box: 0.3771, obj: 3.8315, cls: 0.4902)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 36/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.1522 (box: 0.3078, obj: 3.3093, cls: 0.3040)\n",
            "Val Loss: 5.8759 (box: 0.3612, obj: 3.7546, cls: 0.3154)\n",
            "LR: 0.001000\n",
            "2/10 - No improvement\n",
            "\n",
            "Epoch 37/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.1197 (box: 0.3012, obj: 3.2863, cls: 0.3277)\n",
            "Val Loss: 5.5156 (box: 0.3435, obj: 3.6262, cls: 0.1718)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 5.5156)\n",
            "\n",
            "Epoch 38/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.0516 (box: 0.2931, obj: 3.3286, cls: 0.2575)\n",
            "Val Loss: 5.6663 (box: 0.3627, obj: 3.5531, cls: 0.2998)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 39/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.0271 (box: 0.2994, obj: 3.2555, cls: 0.2747)\n",
            "Val Loss: 6.8009 (box: 0.3781, obj: 3.8274, cls: 1.0831)\n",
            "LR: 0.001000\n",
            "2/10 - No improvement\n",
            "\n",
            "Epoch 40/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 5.0415 (box: 0.2995, obj: 3.2540, cls: 0.2900)\n",
            "Val Loss: 5.5860 (box: 0.3735, obj: 3.5978, cls: 0.1206)\n",
            "LR: 0.001000\n",
            "3/10 - No improvement\n",
            "\n",
            "Epoch 41/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.9772 (box: 0.2938, obj: 3.2725, cls: 0.2358)\n",
            "Val Loss: 5.3744 (box: 0.3476, obj: 3.5243, cls: 0.1122)\n",
            "LR: 0.001000\n",
            "Best model updated! (Val Loss: 5.3744)\n",
            "\n",
            "Epoch 42/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.9440 (box: 0.2951, obj: 3.1796, cls: 0.2891)\n",
            "Val Loss: 5.4496 (box: 0.3508, obj: 3.5555, cls: 0.1402)\n",
            "LR: 0.001000\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 43/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.7940 (box: 0.2836, obj: 3.1672, cls: 0.2088)\n",
            "Val Loss: 5.7012 (box: 0.3544, obj: 3.6864, cls: 0.2427)\n",
            "LR: 0.001000\n",
            "2/10 - No improvement\n",
            "\n",
            "Epoch 44/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.8963 (box: 0.2913, obj: 3.1770, cls: 0.2627)\n",
            "Val Loss: 5.5616 (box: 0.3403, obj: 3.5860, cls: 0.2743)\n",
            "LR: 0.001000\n",
            "3/10 - No improvement\n",
            "\n",
            "Epoch 45/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.8343 (box: 0.2799, obj: 3.1450, cls: 0.2897)\n",
            "Val Loss: 5.5965 (box: 0.3513, obj: 3.5398, cls: 0.3000)\n",
            "LR: 0.000500\n",
            "4/10 - No improvement\n",
            "\n",
            "Epoch 46/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.5054 (box: 0.2594, obj: 3.0098, cls: 0.1985)\n",
            "Val Loss: 5.1935 (box: 0.3167, obj: 3.4778, cls: 0.1322)\n",
            "LR: 0.000500\n",
            "Best model updated! (Val Loss: 5.1935)\n",
            "\n",
            "Epoch 47/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.4199 (box: 0.2471, obj: 3.0119, cls: 0.1724)\n",
            "Val Loss: 5.2672 (box: 0.3216, obj: 3.5089, cls: 0.1503)\n",
            "LR: 0.000500\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 48/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.3843 (box: 0.2482, obj: 2.9524, cls: 0.1910)\n",
            "Val Loss: 5.2085 (box: 0.3117, obj: 3.4386, cls: 0.2116)\n",
            "LR: 0.000500\n",
            "2/10 - No improvement\n",
            "\n",
            "Epoch 49/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.2715 (box: 0.2432, obj: 2.9298, cls: 0.1255)\n",
            "Val Loss: 5.1952 (box: 0.3085, obj: 3.4640, cls: 0.1885)\n",
            "LR: 0.000500\n",
            "3/10 - No improvement\n",
            "\n",
            "Epoch 50/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.2233 (box: 0.2384, obj: 2.8954, cls: 0.1360)\n",
            "Val Loss: 5.4507 (box: 0.3116, obj: 3.5720, cls: 0.3205)\n",
            "LR: 0.000250\n",
            "4/10 - No improvement\n",
            "\n",
            "Epoch 51/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.1208 (box: 0.2318, obj: 2.8414, cls: 0.1205)\n",
            "Val Loss: 5.0237 (box: 0.2960, obj: 3.4074, cls: 0.1362)\n",
            "LR: 0.000250\n",
            "Best model updated! (Val Loss: 5.0237)\n",
            "\n",
            "Epoch 52/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.0162 (box: 0.2246, obj: 2.7757, cls: 0.1175)\n",
            "Val Loss: 5.0783 (box: 0.2994, obj: 3.4191, cls: 0.1622)\n",
            "LR: 0.000250\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 53/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 4.0179 (box: 0.2241, obj: 2.7902, cls: 0.1075)\n",
            "Val Loss: 5.2201 (box: 0.3086, obj: 3.4623, cls: 0.2146)\n",
            "LR: 0.000250\n",
            "2/10 - No improvement\n",
            "\n",
            "Epoch 54/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.9387 (box: 0.2219, obj: 2.7278, cls: 0.1015)\n",
            "Val Loss: 5.0668 (box: 0.3085, obj: 3.4066, cls: 0.1175)\n",
            "LR: 0.000250\n",
            "3/10 - No improvement\n",
            "\n",
            "Epoch 55/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.9361 (box: 0.2206, obj: 2.7223, cls: 0.1108)\n",
            "Val Loss: 5.0371 (box: 0.2999, obj: 3.3931, cls: 0.1445)\n",
            "LR: 0.000125\n",
            "4/10 - No improvement\n",
            "\n",
            "Epoch 56/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.8027 (box: 0.2111, obj: 2.6601, cls: 0.0872)\n",
            "Val Loss: 5.0251 (box: 0.2978, obj: 3.4313, cls: 0.1047)\n",
            "LR: 0.000125\n",
            "5/10 - No improvement\n",
            "\n",
            "Epoch 57/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.8706 (box: 0.2158, obj: 2.6677, cls: 0.1240)\n",
            "Val Loss: 5.0381 (box: 0.3005, obj: 3.4238, cls: 0.1119)\n",
            "LR: 0.000125\n",
            "6/10 - No improvement\n",
            "\n",
            "Epoch 58/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.8144 (box: 0.2108, obj: 2.6743, cls: 0.0860)\n",
            "Val Loss: 5.0425 (box: 0.3041, obj: 3.4224, cls: 0.0994)\n",
            "LR: 0.000125\n",
            "7/10 - No improvement\n",
            "\n",
            "Epoch 59/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.7409 (box: 0.2114, obj: 2.6060, cls: 0.0778)\n",
            "Val Loss: 5.0702 (box: 0.3033, obj: 3.4263, cls: 0.1275)\n",
            "LR: 0.000063\n",
            "8/10 - No improvement\n",
            "\n",
            "Epoch 60/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.7287 (box: 0.2074, obj: 2.5939, cls: 0.0979)\n",
            "Val Loss: 5.0097 (box: 0.2982, obj: 3.4164, cls: 0.1021)\n",
            "LR: 0.000063\n",
            "Best model updated! (Val Loss: 5.0097)\n",
            "\n",
            "Epoch 61/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.7134 (box: 0.2044, obj: 2.5966, cls: 0.0947)\n",
            "Val Loss: 4.9967 (box: 0.2971, obj: 3.3838, cls: 0.1276)\n",
            "LR: 0.000063\n",
            "Best model updated! (Val Loss: 4.9967)\n",
            "\n",
            "Epoch 62/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.6912 (box: 0.2066, obj: 2.5844, cls: 0.0740)\n",
            "Val Loss: 5.0614 (box: 0.2992, obj: 3.4488, cls: 0.1168)\n",
            "LR: 0.000063\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 63/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.7089 (box: 0.2044, obj: 2.6072, cls: 0.0796)\n",
            "Val Loss: 5.0499 (box: 0.2994, obj: 3.4268, cls: 0.1263)\n",
            "LR: 0.000063\n",
            "2/10 - No improvement\n",
            "\n",
            "Epoch 64/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.6516 (box: 0.1988, obj: 2.5603, cls: 0.0972)\n",
            "Val Loss: 4.9796 (box: 0.2943, obj: 3.3695, cls: 0.1385)\n",
            "LR: 0.000063\n",
            "Best model updated! (Val Loss: 4.9796)\n",
            "\n",
            "Epoch 65/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.6674 (box: 0.2009, obj: 2.5780, cls: 0.0848)\n",
            "Val Loss: 5.0121 (box: 0.2974, obj: 3.3938, cls: 0.1314)\n",
            "LR: 0.000063\n",
            "1/10 - No improvement\n",
            "\n",
            "Epoch 66/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.6242 (box: 0.2001, obj: 2.5387, cls: 0.0851)\n",
            "Val Loss: 4.9968 (box: 0.2973, obj: 3.3990, cls: 0.1113)\n",
            "LR: 0.000063\n",
            "2/10 - No improvement\n",
            "\n",
            "Epoch 67/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.6448 (box: 0.2029, obj: 2.5493, cls: 0.0811)\n",
            "Val Loss: 4.9881 (box: 0.2927, obj: 3.4011, cls: 0.1236)\n",
            "LR: 0.000063\n",
            "3/10 - No improvement\n",
            "\n",
            "Epoch 68/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.5686 (box: 0.1956, obj: 2.5132, cls: 0.0774)\n",
            "Val Loss: 5.0625 (box: 0.2983, obj: 3.4134, cls: 0.1575)\n",
            "LR: 0.000031\n",
            "4/10 - No improvement\n",
            "\n",
            "Epoch 69/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.5728 (box: 0.1934, obj: 2.5262, cls: 0.0794)\n",
            "Val Loss: 5.0332 (box: 0.2972, obj: 3.3888, cls: 0.1585)\n",
            "LR: 0.000031\n",
            "5/10 - No improvement\n",
            "\n",
            "Epoch 70/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.5977 (box: 0.1943, obj: 2.5431, cls: 0.0833)\n",
            "Val Loss: 5.0713 (box: 0.2989, obj: 3.4389, cls: 0.1381)\n",
            "LR: 0.000031\n",
            "6/10 - No improvement\n",
            "\n",
            "Epoch 71/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.5724 (box: 0.1978, obj: 2.4945, cls: 0.0890)\n",
            "Val Loss: 5.0832 (box: 0.3017, obj: 3.4406, cls: 0.1341)\n",
            "LR: 0.000031\n",
            "7/10 - No improvement\n",
            "\n",
            "Epoch 72/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.5963 (box: 0.1963, obj: 2.5320, cls: 0.0827)\n",
            "Val Loss: 5.1092 (box: 0.3007, obj: 3.4668, cls: 0.1387)\n",
            "LR: 0.000016\n",
            "8/10 - No improvement\n",
            "\n",
            "Epoch 73/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.5413 (box: 0.1953, obj: 2.4925, cls: 0.0721)\n",
            "Val Loss: 5.0557 (box: 0.3022, obj: 3.4249, cls: 0.1198)\n",
            "LR: 0.000016\n",
            "9/10 - No improvement\n",
            "\n",
            "Epoch 74/75\n",
            "----------------------------------------------------------------------\n",
            "Train Loss: 3.5772 (box: 0.1942, obj: 2.5305, cls: 0.0757)\n",
            "Val Loss: 5.0241 (box: 0.2986, obj: 3.4056, cls: 0.1256)\n",
            "LR: 0.000016\n",
            "10/10 - No improvement\n",
            "\n",
            "Early Stopping at epoch 74\n",
            "\n",
            "Training completed: 32m 14s\n",
            "Best Val Loss: 4.9796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IOU (Intersection Over Union) is a fundamental metric in object detection that measures how accurately a predicted box encompasses the actual box. This function calculates an IOU matrix by dividing the intersection area between two sets of boxes (boxes1 and boxes2) by the union area. A high IOU (0.5 and above) indicates a successful prediction and is a requirement for being considered True Positive in mAP calculations."
      ],
      "metadata": {
        "id": "ulEvXFCdWHqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def box_iou(boxes1, boxes2):\n",
        "\n",
        "    area1 = (boxes1[:,2] - boxes1[:,0]) * (boxes1[:,3] - boxes1[:,1])\n",
        "    area2 = (boxes2[:,2] - boxes2[:,0]) * (boxes2[:,3] - boxes2[:,1])\n",
        "\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)\n",
        "    inter = wh[:,:,0] * wh[:,:,1]\n",
        "\n",
        "    union = area1[:, None] + area2 - inter\n",
        "    return inter / union"
      ],
      "metadata": {
        "id": "AFxAzMJQfxiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `calculate_ap_per_class` function calculates the Average Precision (AP) by determining the area under the Precision-Recall curve for a single class. The model's predictions are ranked according to their confidence score, True Positive (TP) and False Positive (FP) decisions are made, and Precision and Recall values ​​are extracted. This function calculates the AP value using an 11-point sampling method (0.0 to 1.0 with 0.1 intervals)."
      ],
      "metadata": {
        "id": "XQxl8QAWWRHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ap_per_class(pred_boxes, pred_scores, true_boxes, iou_threshold=0.5):\n",
        "    if len(pred_boxes) == 0 or len(true_boxes) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    order = torch.argsort(pred_scores, descending=True)\n",
        "    pred_boxes = pred_boxes[order]\n",
        "    pred_scores = pred_scores[order]\n",
        "\n",
        "    ious = box_iou(pred_boxes, true_boxes)\n",
        "    gt_matched = torch.zeros(len(true_boxes), dtype=torch.bool)\n",
        "\n",
        "    tp, fp = [], []\n",
        "    for i in range(len(pred_boxes)):\n",
        "        iou_row = ious[i]\n",
        "        iou_max = iou_row.max()\n",
        "        j = int(iou_row.argmax())\n",
        "        if iou_max >= iou_threshold and not gt_matched[j]:\n",
        "            tp.append(1); fp.append(0)\n",
        "            gt_matched[j] = True\n",
        "        else:\n",
        "            tp.append(0); fp.append(1)\n",
        "\n",
        "    tp = torch.tensor(tp).cumsum(0)\n",
        "    fp = torch.tensor(fp).cumsum(0)\n",
        "    recalls = tp / max(len(true_boxes), 1)\n",
        "    precisions = tp / (tp + fp + 1e-6)\n",
        "\n",
        "    precisions = torch.clamp(precisions, max=1.0)\n",
        "\n",
        "    ap = 0.0\n",
        "    for t in torch.linspace(0,1,11):\n",
        "        mask = recalls >= t\n",
        "        if mask.any():\n",
        "            p = torch.max(precisions[mask]).item()\n",
        "            p = min(1.0, p)\n",
        "        else:\n",
        "            p = 0.0\n",
        "        ap += p/11.0\n",
        "\n",
        "    ap = min(1.0, ap)\n",
        "    return ap\n"
      ],
      "metadata": {
        "id": "1OYtF7bhfzKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `evaluate_map` function calculates the mAP (Mean Average Precision) score, which is the final metric of the model. Model predictions are made for all images in the validation data loader, and Non-Maximum Suppression (NMS) is applied to these predictions. Then, AP is calculated separately for each error class, and the average of the AP values ​​of all classes is taken to obtain the mAP@0.5 score (64.16%)."
      ],
      "metadata": {
        "id": "yJ2dsCfKWxgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_map(model, dataloader, class_names, device, conf_threshold=0.2, iou_threshold=0.5):\n",
        "    model.eval()\n",
        "    aps = {cls: [] for cls in class_names}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, boxes_list, labels_list in dataloader:\n",
        "            images = images.to(device)\n",
        "            preds = model(images)\n",
        "\n",
        "            pred_boxes, pred_scores, pred_labels = decode_predictions(\n",
        "                preds, model.anchors, confidence_threshold=conf_threshold\n",
        "            )\n",
        "\n",
        "            for i in range(len(images)):\n",
        "                gt_boxes = boxes_list[i].to(device)\n",
        "                gt_labels = labels_list[i].to(device)\n",
        "\n",
        "                for cls_idx, cls_name in enumerate(class_names):\n",
        "                    pm = (pred_labels[i] == cls_idx)\n",
        "                    gm = (gt_labels == cls_idx)\n",
        "                    if pm.any() and gm.any():\n",
        "                        ap = calculate_ap_per_class(\n",
        "                            pred_boxes[i][pm], pred_scores[i][pm], gt_boxes[gm], iou_threshold\n",
        "                        )\n",
        "                        aps[cls_name].append(ap)\n",
        "\n",
        "    final_aps = {cls: (sum(vals)/len(vals) if len(vals) > 0 else 0.0) for cls, vals in aps.items()}\n",
        "    map_score = sum(final_aps.values())/len(final_aps)\n",
        "\n",
        "    print(\"AP per class:\", {k: f\"{v:.4f}\" for k,v in final_aps.items()})\n",
        "    print(f\"mAP@{iou_threshold}: {map_score:.4f} ({map_score*100:.2f}%)\")\n",
        "    return map_score, final_aps"
      ],
      "metadata": {
        "id": "w-w3JuXVf2B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mAP evaluation\n",
        "map_score, class_aps = evaluate_map(\n",
        "    model=trained_model,\n",
        "    dataloader=val_det_loader,\n",
        "    class_names=class_names,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"AP per class:\", {k: f\"{v:.4f}\" for k, v in class_aps.items()})\n",
        "print(f\"mAP@0.5: {map_score:.4f} ({map_score*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzCs4EkNx0my",
        "outputId": "1df14404-20f3-48b9-901b-8bed433abb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AP per class: {'crazing': '0.6025', 'inclusion': '0.7316', 'patches': '0.7721', 'pitted_surface': '0.8230', 'rolled-in_scale': '0.5617', 'scratches': '0.3589'}\n",
            "mAP@0.5: 0.6416 (64.16%)\n",
            "AP per class: {'crazing': '0.6025', 'inclusion': '0.7316', 'patches': '0.7721', 'pitted_surface': '0.8230', 'rolled-in_scale': '0.5617', 'scratches': '0.3589'}\n",
            "mAP@0.5: 0.6416 (64.16%)\n"
          ]
        }
      ]
    }
  ]
}